**SENG 438- Software Testing, Reliability, and Quality**

**Lab. Report \#5 – Software Reliability Assessment**

| Group \#: 11      |     |
| ----------------- | --- |
| Student Names:    |     |
| Ryan Khryss Obiar |
| Armin Sandhu      |
| Parsa Karagari    |
| Jayden Mikuclcik  |

GOOGLE DOC LINK: https://docs.google.com/document/d/19oxT0jhSFFHVH6MKPUqYV33OBNyCRsN84abtW58VVvg/edit

# Introduction

For this assignment our group is tasked to dive into both Reliability growth testing and Reliability assessment using Reliability Demonstration Chart (RDC), both of which are essential practices to understand in order to have a proper grasp on the Software testing that gets applied in industry. The first part of the lab is focussed on Reliability growth testing in which we will be using START (our chosen tool) to measure the failure rate MTTF and the reliability of the system under test through analyzing the test data provided. The end goal of this half of the lab is to gain an understanding of reliability growth testing and why it is useful. The latter part of this lab focuses on assessment using a reliability demonstration chart - which is an efficient way of checking whether the target failure rate or MTTF is met or not. It will give us the chance to familiarize ourselves with RDC and analyze the testing for a given MTTF of the system under test through plotting the test data. Overall this assignment will  be used as a great introduction to the assessment of failure data.

# Assessment Using Reliability Growth Testing

Please Refer to the Google Doc or the PDF for the Reliability Growth Testing Section:
https://docs.google.com/document/d/19oxT0jhSFFHVH6MKPUqYV33OBNyCRsN84abtW58VVvg/edit?usp=sharing

# Assessment Using Reliability Demonstration Chart

Please Refer to the Google Doc or the PDF for the Reliability Demonstration Chart Section:
https://docs.google.com/document/d/19oxT0jhSFFHVH6MKPUqYV33OBNyCRsN84abtW58VVvg/edit?usp=sharing

# Comparison of Results

The key differences of the two techniques used, are discussed in the next section.

As for our results and testing: 
In our first part, we found that the “S distribution” and “Discrete Weibull” were to be the best fits for modeling the failures. In our second part, instead of docusing on the failures over time, we focused on whether the system met different reliability targets set by us. We plotted RDC plots for MTTFmin, twice MTTFmin, and half MTTFmin which all indicated different risk acceptance levels (accepted, and neutral).
The Reliability Growth Testing method that we used, showed that it was more sensitive to variations of the data, as the intensity graph showed fluctuations. On the other hand. The RDC did not measure sensitivity, but it assessed the measure due to predeterminced values and conditions.
In our part 2 of RDC, we noticed how decreasing the failures per second, would also decrease the MTTFs, pushing the failure data towards the failure data.

Furthermore, RDC provided us with a clear, binary outcome that is more straightforward in making decisions when compared to our Reliability Growth Testing method in part 1.
Overall, both parts showed valuable results and information to us, with Part 1 providing a more dynamic, retailed look at reliability growth and Part 2 providing us with a more straightforward and static information. We also noticed how RDC may serve as a “validation step” before deployment or decisions, as it is a binary decision of pass/fail, whereas, the growth testing results showed us how it can be used for ongoing system reliability improvements.


# Discussion on Similarity and Differences of the Two Techniques

In discussing the similarities and differences between Reliability Growth Testing and Assessment Using Reliability Demonstration Chart (RDC), it's crucial to consider their distinct objectives, methodologies, and applications within reliability assessment. Reliability Growth Testing aims to iteratively assess and improve the reliability of a system over time through continuous testing. This approach is good for dynamic systems undergoing ongoing development. In contrast, RDC focuses on demonstrating whether a system meets predefined reliability targets within a limited testing period. It assesses the system's reliability based on collected failure data at specific time points. Methodologically, Reliability Growth Testing involves iterative testing processes, where failure data is continuously collected, analyzed, and utilized to refine reliability estimates using things like statistical models (Geometric, Laplace, etc.). On the other hand, RDC utilizes graphical representations, such as charts, to visualize the relationship between failure data and predefined reliability targets. It assesses whether the observed failure data aligns with the expected reliability levels. While both techniques share the common goal of assessing and enhancing system reliability, they differ significantly in their methodologies and objectives. Understanding these differences is essential for selecting the most appropriate approach based on project requirements and constraints, ensuring effective reliability assessment and improvement strategies are implemented.

# How the team work/effort was divided and managed

For this lab specifically, we decided to work on the whole lab to ensure that our understanding of failure data assessment was sound and that everyone was able to get some hands-on experience with the tools used in this lab. We had issues running the tools and executing them with the data provided to us on the Macbook computers in the group so working together in this manner really helped everyone in the group understand each part of this lab. The environment was one in which we could ask questions, and especially one that ensured we were all on the same page which we believe was paramount to our understanding of Reliability Growth Testing and assessment using a Reliability Demonstration Chart.

# Difficulties encountered, challenges overcome, and lessons learned

During our time completing this lab, as mentioned above a significant challenge that rose was that not all team members could execute the given tools with the data provided, ie. those on Macbook computers. This forced us to rethink our usual divide-and-conquer strategy. Instead, we opted for a collaborative approach, where everyone pitched in to find a solution. ANother difficulty was our unfamiliarity with the software. We had to invest extra time to understand its ins and outs fully. This involved delving into tutorials, documentation, and collaborating with others to grasp its functionalities effectively. However, through teamwork and perseverance, we managed to overcome these hurdles. This experience emphasized the importance of adaptability and teamwork in navigating challenges. It also strengthened our understanding of the analysis process and fostered closer bonds within the team.

# Comments/feedback on the lab itself

Overall feedback on this lab was very positive as we got to use and understand a technology used in the field. This coupled with the critical thinking it took to come up with each method we used meant to our group a lab that is essential to complete in order to round out our skills as software engineers. The general consensus of our group was that the lab was very well designed from the content we had to implement to the workflow of the readme being laid out very well allowing for a great lab experience